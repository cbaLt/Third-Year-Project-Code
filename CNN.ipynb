{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tnpPytWMhW2g"
      },
      "source": [
        "# Formatting Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qs97ICK9M4yq"
      },
      "outputs": [],
      "source": [
        "import scipy.io as sio\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix, ConfusionMatrixDisplay\n",
        "from skimage.feature import hog\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.svm import SVC\n",
        "import os\n",
        "import scipy.sparse as sp\n",
        "import scipy.io as sio\n",
        "import time\n",
        "import cuml\n",
        "from cuml.svm import SVC as cuSVC\n",
        "import cudf\n",
        "from sklearn.decomposition import PCA\n",
        "import scipy.stats\n",
        "import librosa\n",
        "from cuml.neighbors import KNeighborsClassifier as cuKNN\n",
        "import cupy as cp\n",
        "import h5py\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import gc\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Dense, Flatten, BatchNormalization, Dropout, Activation, ReLU, Input, LeakyReLU\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint, LearningRateScheduler\n",
        "from tensorflow.keras import regularizers\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "import tensorflow as tf\n",
        "from sklearn.utils import class_weight\n",
        "from scipy.stats import mode\n",
        "import string\n",
        "\n",
        "# to split data like papers for reproducibility\n",
        "def split_trials(trial_data, num_gestures, training_trials, test_trials):\n",
        "\n",
        "    # calculate number of trials from arrays given. Create 3D array to store indices for each trial, for each gesture\n",
        "    num_trials = len(training_trials) + len(test_trials)\n",
        "    trial_index = np.zeros((num_trials, num_gestures, 2))\n",
        "\n",
        "    curr_label = 0                                     # label indexed from 0, so gesture 1 is label 0 etc.\n",
        "    for i in range(1, len(trial_data)):\n",
        "        curr_trial = trial_data[i]- 1              # minus 1 to make zero indexed - trial 1 is 0 etc.\n",
        "        prev_trial = trial_data[i-1] - 1\n",
        "\n",
        "        if curr_trial != prev_trial:                   # store index at beginning and end of trial\n",
        "\n",
        "            # zero indexed\n",
        "            if curr_trial == 0:                                 # if trial is zero (trial 1), that means it is a new gesture (label)\n",
        "                trial_index[prev_trial][curr_label][1] = i      # store index for beginning of that trial, for that gesture\n",
        "                curr_label += 1                                 # update gesture (label) by 1\n",
        "\n",
        "                trial_index[curr_trial][curr_label][0] = i      # store index for end of that trial - it is an index for range() function, so this value will not be included, e.g. 2017, means up to 2016\n",
        "\n",
        "            else:\n",
        "                trial_index[prev_trial][curr_label][1] = i      # same as before, except there is no need to update gesture (label), as trial is between 1-6 (or 0-5 zero-indexed)\n",
        "                trial_index[curr_trial][curr_label][0] = i\n",
        "\n",
        "    # update last trial, last gesture (label) beginning index, as it is wrong!!\n",
        "    trial_index[(num_trials-1)][(num_gestures-1)][0] = trial_index[(num_trials-2)][(num_gestures-1)][1]\n",
        "\n",
        "    return trial_index\n",
        "\n",
        "def split_labels(index_info, label_data, training_trials, test_trials):\n",
        "\n",
        "    training_labels, test_labels = [], []\n",
        "\n",
        "    # use the indices found to create arrays for training data (labels and features)\n",
        "    for j in training_trials:\n",
        "        trial_num = j-1\n",
        "\n",
        "        for indices in index_info[trial_num]:\n",
        "                beg, end = int(indices[0]), int(indices[1])\n",
        "\n",
        "                # slice the training labels and features\n",
        "                training_labels = np.append(training_labels, label_data[beg:end])\n",
        "\n",
        "    # use the indices found to slice testing features and labels\n",
        "    for j in test_trials:\n",
        "        trial_num = j-1\n",
        "\n",
        "        for indices in index_info[trial_num]:\n",
        "\n",
        "                beg, end = int(indices[0]), int(indices[1])\n",
        "                test_labels = np.append(test_labels, label_data[beg:end])\n",
        "\n",
        "    return training_labels, test_labels\n",
        "\n",
        "def split_time_series(index_info, time_series_arr, electrode_num, train_len, test_len, training_trials, test_trials):\n",
        "\n",
        "    train_split_series = np.zeros((electrode_num, train_len))\n",
        "    test_split_series = np.zeros((electrode_num, test_len))\n",
        "\n",
        "    for num in range(electrode_num):\n",
        "\n",
        "      training_series, test_series= [], []\n",
        "\n",
        "      # use the indices found to create arrays for training data\n",
        "      for j in training_trials:\n",
        "          trial_num = j-1\n",
        "\n",
        "          for indices in index_info[trial_num]:\n",
        "              beg, end = int(indices[0]), int(indices[1])\n",
        "\n",
        "              # slice the training labels and features\n",
        "              training_series = np.append(training_series, time_series_arr[num, beg:end])\n",
        "\n",
        "      train_split_series[num] = training_series\n",
        "\n",
        "      # use the indices found to slice testing features and labels\n",
        "      for i in test_trials:\n",
        "          trial_num = i-1\n",
        "\n",
        "          for indices in index_info[trial_num]:\n",
        "\n",
        "              beg, end = int(indices[0]), int(indices[1])\n",
        "              test_series = np.append(test_series, time_series_arr[num, beg:end])\n",
        "\n",
        "\n",
        "      test_split_series[num] = test_series\n",
        "\n",
        "    return train_split_series, test_split_series\n",
        "\n",
        "def rolling_window(arr, window_len, step, arr_dimension):\n",
        "\n",
        "    if arr_dimension == 2:\n",
        "        num_windows = (arr.shape[0] - window_len) // step + 1\n",
        "        windows = np.zeros((num_windows, window_len, arr.shape[1]), dtype=arr.dtype)\n",
        "    elif arr_dimension == 1:\n",
        "        num_windows = (len(arr) - window_len) // step + 1\n",
        "        windows = np.zeros((num_windows, window_len), dtype=arr.dtype)\n",
        "\n",
        "    for i in range(num_windows):\n",
        "        start = i * step\n",
        "        end = start + window_len\n",
        "        windows[i] = arr[start:end]\n",
        "\n",
        "    return windows\n",
        "\n",
        "def rolling_window_electrodes(arr, window_len, step):\n",
        "\n",
        "    num_windows = (arr.shape[1] - window_len) // step + 1\n",
        "    windows = np.zeros((arr.shape[0], num_windows, window_len), dtype=arr.dtype)\n",
        "\n",
        "    for i in range(num_windows):\n",
        "      start = i * step\n",
        "      end = start + window_len\n",
        "      windows[:, i] = arr[:, start:end]  # Slice along columns\n",
        "\n",
        "    return windows\n",
        "\n",
        "def one_hot_encoder(labels, gestures):\n",
        "\n",
        "        one_hot = np.zeros((len(labels), gestures))\n",
        "\n",
        "        for index, value in enumerate(labels):\n",
        "            label_encode = int(value)\n",
        "            one_hot[index][label_encode] = 1\n",
        "\n",
        "        return one_hot\n",
        "\n",
        "class LocallyConnectedLayer(layers.Layer):\n",
        "    def __init__(self, filters, kernel_size, **kwargs):\n",
        "        super(LocallyConnectedLayer, self).__init__(**kwargs)\n",
        "        self.filters = filters\n",
        "        self.kernel_size = kernel_size\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        # Initialize the weights for each spatial location\n",
        "        self.kernel = self.add_weight(\n",
        "            name='kernel',\n",
        "            shape=(self.kernel_size[0], self.kernel_size[1], input_shape[-1], self.filters),\n",
        "            initializer='glorot_uniform',\n",
        "            trainable=True\n",
        "        )\n",
        "        super(LocallyConnectedLayer, self).build(input_shape)\n",
        "\n",
        "    def call(self, inputs):\n",
        "        # Perform the \"local\" convolution manually without weight sharing\n",
        "        return tf.nn.conv2d(inputs, self.kernel, strides=[1, 1, 1, 1], padding='SAME')\n",
        "\n",
        "def lr_schedule(epoch):\n",
        "    initial_lr = 0.002  # Initial learning rate\n",
        "    drop_factor = 0.5  # Factor by which to reduce the learning rate\n",
        "    epochs_drop = 20  # Interval (in epochs) to reduce the learning rate\n",
        "    return initial_lr * (drop_factor ** (epoch // epochs_drop))\n",
        "\n",
        "def generate_signal_permutations(Ns):\n",
        "\n",
        "    base = 36\n",
        "    SIS = \"\"  # String to store signal indices\n",
        "    NSIS = 1  # Initial length of SIS\n",
        "\n",
        "    # Start with the first signal sequence (hexadecimal '1')\n",
        "    #SIS = format(1, 'X')  # SIS starts with '1' in hexadecimal\n",
        "    SIS = int_to_base(1, base)\n",
        "\n",
        "\n",
        "    i = 1\n",
        "    j = i + 1\n",
        "\n",
        "    # Apply the permutation logic as per the original algorithm\n",
        "    while i != j:\n",
        "        if j > Ns:\n",
        "            j = 1  # Wrap around if j exceeds Ns\n",
        "        #elif format(i, 'X') + format(j, 'X') not in SIS and format(j, 'X') + format(i, 'X') not in SIS:  # If pair not in SIS\n",
        "        elif int_to_base(i, base) + int_to_base(j, base) not in SIS and int_to_base(j, base) + int_to_base(i, base) not in SIS:\n",
        "            # Append the j-th signal index to SIS (in hexadecimal)\n",
        "            #SIS += format(j, 'X')  # Add j to the string in hexadecimal\n",
        "            SIS += int_to_base(j, base)\n",
        "            NSIS += 1  # Increment NSIS\n",
        "            i = j  # Update i to current j\n",
        "            j = i + 1  # Update j to i + 1\n",
        "        else:\n",
        "            j += 1  # Move j to the next signal sequence\n",
        "    # Return the final Signal Index String (SIS) as a list of integers and NSIS\n",
        "    #return [int(x, 16) for x in SIS], NSIS\n",
        "    return [int(x, base) for x in SIS], NSIS\n",
        "\n",
        "def int_to_base(n, base):\n",
        "\n",
        "    digits = \"0123456789ABCDEFGHIJKLMNOPQRSTUVWXYZ\"\n",
        "    # Create a string with all digits and letters for bases up to 36\n",
        "    digits = string.digits + string.ascii_lowercase\n",
        "    if n == 0:\n",
        "        return digits[0]\n",
        "\n",
        "    result = []\n",
        "    while n:\n",
        "        result.append(digits[n % base])\n",
        "        n //= base\n",
        "    return ''.join(reversed(result))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UJTKipzfPVnO"
      },
      "source": [
        "# TO RUN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2zNT6pi3OnmX",
        "outputId": "a77f2b2c-66b6-49c5-ecd7-06c93d23487f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "from google.colab import drive\n",
        "# ADJUST ACCORDINLY\n",
        "database = 'DB4'\n",
        "\n",
        "# ADJUST ACCORDINLY\n",
        "evaluation = 'HHT'\n",
        "\n",
        "# ADJUST DEPNDING ON WHAT FEATURES\n",
        "num_features = 3\n",
        "\n",
        "# ADJUST DEPNDING ON Signal Image or not\n",
        "signal_image = True\n",
        "\n",
        "# ok, so E2 and E3 adjusted means to adjust them gesture back to 1-N gestures, otherwise it is say 18-40 but depending on what gesture set went first\n",
        "data_dict = {\n",
        "        'DB1': {'E1': 13, 'E2': 18, 'E3': 24, 'E1_adjusted': 0, 'E2_adjusted': 0, 'E3_adjusted': 0, 'fs': 100, 'electrodes': 10, 'subjects': 27, 'train': [1, 3, 4, 6, 7, 8, 9], 'test': [2, 5, 10], 'window length': 20, 'step': 1},\n",
        "        'DB2': {'E1': 18, 'E2': 24, 'E3': 10, 'E1_adjusted': 0, 'E2_adjusted': -17, 'E3_adjusted': -40, 'fs': 100, 'electrodes': 12, 'subjects': 40, 'train': [1, 3, 4, 6], 'test': [2, 5], 'window length': 20, 'step': 1},\n",
        "        'DB3': {'E1': 18, 'E2': 24, 'E3': 10, 'E1_adjusted': 0, 'E2_adjusted': -17, 'E3_adjusted': -40, 'fs': 200, 'electrodes': 12, 'subjects': 11, 'train': [1, 3, 4, 6], 'test': [2, 5], 'window length': 40, 'step': 2},\n",
        "        'DB4': {'E1': 13, 'E2': 18, 'E3': 24, 'E1_adjusted': 0, 'E2_adjusted': 0, 'E3_adjusted': 0,'fs': 200, 'electrodes': 12, 'subjects': 10, 'train': [1, 3, 4, 6], 'test': [2, 5], 'window length': 40, 'step': 2},\n",
        "        'DB5': {'E1': 13, 'E2': 18, 'E3': 24, 'E1_adjusted': 0, 'E2_adjusted': 0, 'E3_adjusted': 0,'fs': 200, 'electrodes': 16, 'subjects': 10, 'train': [1, 3, 4, 6], 'test': [2, 5], 'window length': 40, 'step': 2}\n",
        "        }\n",
        "\n",
        "num_subjects = data_dict[database]['subjects']\n",
        "fs = data_dict[database]['fs']\n",
        "num_electrodes = data_dict[database]['electrodes']\n",
        "\n",
        "train_trials =  data_dict[database]['train']\n",
        "test_trials = data_dict[database]['test']\n",
        "M, step = data_dict[database]['window length'], data_dict[database]['step']\n",
        "num_freq_bins = int((fs / 2) / (1 / (1/fs * M)))\n",
        "freq_bins = np.linspace(0, fs/2, num_freq_bins)\n",
        "\n",
        "\n",
        "drive.mount('/content/drive')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "d9EGTBCzB6wG",
        "outputId": "b499bbe1-7689-46a2-de6e-c7430a9bed18"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "function ClickConnect(){\n",
              "    console.log(\"Clicked on connect button\");\n",
              "    document.querySelector(\"colab-connect-button\").click()\n",
              "}\n",
              "setInterval(ClickConnect,60000)\n"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "# simulate button click to prevent server going idle\n",
        "%%javascript\n",
        "function ClickConnect(){\n",
        "    console.log(\"Clicked on connect button\");\n",
        "    document.querySelector(\"colab-connect-button\").click()\n",
        "}\n",
        "setInterval(ClickConnect,60000)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LAdPdauyho1F"
      },
      "source": [
        "# Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M-II0F2e2ijE"
      },
      "outputs": [],
      "source": [
        "\n",
        "for exercise in ['E1', 'E2', 'E3']:\n",
        "\n",
        "  label_dict = {f'S{num}': [] for num in range(1, num_subjects+1)}\n",
        "  features_dict = {f'S{num}': [] for num in range(1, num_subjects+1)}\n",
        "  num_gestures = data_dict[database][exercise]\n",
        "\n",
        "  # for all subjects\n",
        "  for subject in range(1,(num_subjects+1)):\n",
        "\n",
        "\n",
        "      # subjects 6 and 7 were not evaulated for DB3\n",
        "      if database == 'DB3':\n",
        "        if subject in [6,7]:\n",
        "          continue\n",
        "\n",
        "      # load labels and features for subject - DB2, DB3, DB5\n",
        "      file = sio.loadmat(f'/content/drive/My Drive/uni/{database}/Electrode Data/S{subject}_{exercise}_A1.mat')\n",
        "\n",
        "      label = file['restimulus'].flatten()\n",
        "      trials = np.int8(file['rerepetition']).flatten()\n",
        "\n",
        "      if database in ['DB3', 'DB4']:\n",
        "        downsample_factor = 10\n",
        "        label = label[::downsample_factor]\n",
        "        trials = trials[::downsample_factor]\n",
        "\n",
        "      # find indexes of where trials begin and end\n",
        "      trial_split_index = split_trials(trials, num_gestures, train_trials, test_trials)\n",
        "      # split labels with trial info\n",
        "      train_labels, test_labels = split_labels(trial_split_index, label, train_trials, test_trials)\n",
        "\n",
        "      # load train features\n",
        "      train_feature_path = f'/content/drive/My Drive/uni/{database}/Features_down/Training_{exercise}_S{subject}_features.h5'\n",
        "      with h5py.File(train_feature_path, 'r') as train_file:\n",
        "          train_mav = train_file['MAV'][:]\n",
        "          train_mavs = train_file['MAVS'][:]\n",
        "          train_wap = train_file['WAP'][:]\n",
        "          train_zcr = train_file['ZC'][:]\n",
        "          train_ssc = train_file['SSC'][:]\n",
        "          train_ar1 = train_file['ar1'][:]\n",
        "          train_ar2 = train_file['ar2'][:]\n",
        "          train_ar3 = train_file['ar3'][:]\n",
        "          train_ar4 = train_file['ar4'][:]\n",
        "          train_wl = train_file['WL'][:]\n",
        "          train_rms = train_file['RMS'][:]\n",
        "          train_ssc = train_file['SSC'][:]\n",
        "          train_var = train_file['VAR'][:]\n",
        "          train_iemg = train_file['IEMG'][:]\n",
        "\n",
        "      # load either HHT or STFT data\n",
        "      if evaluation == 'HHT':\n",
        "          hht_test_path = f'/content/drive/My Drive/uni/{database}/HHT/Test_{exercise}_S{subject}_hht.h5'\n",
        "          hht_train_path = f'/content/drive/My Drive/uni/{database}/HHT/Training_{exercise}_S{subject}_hht.h5'\n",
        "      elif evaluation == 'STFT':\n",
        "          hht_test_path = f'/content/drive/My Drive/uni/{database}/STFT/Test_{exercise}_S{subject}_stft.h5'\n",
        "          hht_train_path = f'/content/drive/My Drive/uni/{database}/STFT/Training_{exercise}_S{subject}_stft.h5'\n",
        "\n",
        "      with h5py.File(hht_train_path, 'r') as hht_file:\n",
        "          train_mean_freq = hht_file['mean freq'][:]\n",
        "          train_skew_freq = hht_file['skew freq'][:]\n",
        "          train_psr = hht_file['psr'][:]\n",
        "          #train_imfs = hht_file['num imfs'][:]\n",
        "          train_peak_freq = hht_file['peak freq'][:]\n",
        "          train_mean_power = hht_file['mean power'][:]\n",
        "          train_kurt_freq = hht_file['kurt freq'][:]\n",
        "          train_var_freq = hht_file['var freq'][:]\n",
        "\n",
        "      with h5py.File(hht_test_path, 'r') as hht_file:\n",
        "          test_mean_freq = hht_file['mean freq'][:]\n",
        "          test_skew_freq = hht_file['skew freq'][:]\n",
        "          test_psr = hht_file['psr'][:]\n",
        "          #test_imfs = hht_file['num imfs'][:]\n",
        "          test_peak_freq = hht_file['peak freq'][:]\n",
        "          test_mean_power = hht_file['mean power'][:]\n",
        "          test_kurt_freq = hht_file['kurt freq'][:]\n",
        "          test_var_freq = hht_file['var freq'][:]\n",
        "\n",
        "      if evaluation == 'HHT':\n",
        "          train_mean_power = np.squeeze(train_mean_power)\n",
        "          train_mean_freq = np.squeeze(train_mean_freq)\n",
        "          train_psr = np.squeeze(train_psr)\n",
        "          test_mean_power = np.squeeze(test_mean_power)\n",
        "          test_mean_freq = np.squeeze(test_mean_freq)\n",
        "          test_psr = np.squeeze(test_psr)\n",
        "\n",
        "      # FEATURE SET 1 - 11 features\n",
        "      #train_matrices = [train_mav, train_mavs, train_wap, train_zcr, train_ar1, train_ar2, train_ar3, train_ar4, train_wl, train_mean_freq, train_psr]\n",
        "\n",
        "      # FEATURE SET 2 - 6 features\n",
        "      #train_matrices = [train_iemg, train_var, train_wap, train_wl, train_ssc, train_zcr]\n",
        "\n",
        "      # FEATURE SET 3 - 3 features\n",
        "      train_matrices = [train_mean_power, train_wl, train_mav]\n",
        "\n",
        "      # reshape array\n",
        "      train_images = np.stack(train_matrices, axis=-1).transpose(1, 0, 2)\n",
        "\n",
        "      # load test features\n",
        "      test_feature_path = f'/content/drive/My Drive/uni/{database}/Features_down/Test_{exercise}_S{subject}_features.h5'\n",
        "      with h5py.File(test_feature_path, 'r') as test_file:\n",
        "          test_mav = test_file['MAV'][:]\n",
        "          test_mavs = test_file['MAVS'][:]\n",
        "          test_wap = test_file['WAP'][:]\n",
        "          test_zcr = test_file['ZC'][:]\n",
        "          test_ssc = test_file['SSC'][:]\n",
        "          test_ar1 = test_file['ar1'][:]\n",
        "          test_ar2 = test_file['ar2'][:]\n",
        "          test_ar3 = test_file['ar3'][:]\n",
        "          test_ar4 = test_file['ar4'][:]\n",
        "          test_wl = test_file['WL'][:]\n",
        "          test_rms = test_file['RMS'][:]\n",
        "          test_ssc = test_file['SSC'][:]\n",
        "          test_var = test_file['VAR'][:]\n",
        "          test_iemg = test_file['IEMG'][:]\n",
        "\n",
        "      # FEATURE SET 1 - 11 features\n",
        "      #test_matrices = [test_iemg, test_var, test_wap, test_wl, test_ssc, test_zcr]\n",
        "\n",
        "      # FEATURE SET 2 - 6 features\n",
        "      #test_matrices = [test_mav, test_mavs, test_wap, test_zcr, test_ar1, test_ar2, test_ar3, test_ar4, test_wl, test_mean_freq, test_psr]\n",
        "\n",
        "      # FEATURE Set 3 - 3 features\n",
        "      test_matrices = [test_mean_power, test_wl, test_mav]\n",
        "\n",
        "      # reshape array\n",
        "      test_images = np.stack(test_matrices, axis=-1).transpose(1, 0, 2)\n",
        "\n",
        "      # if testing different permutations of rows\n",
        "      if signal_image == True:\n",
        "\n",
        "        # use algorithm to get row order\n",
        "        Ns = train_images.shape[1]\n",
        "        SIS, NSIS = generate_signal_permutations(Ns)\n",
        "        print(SIS, NSIS)\n",
        "\n",
        "        # reshape train features to signal image\n",
        "        train_features = np.zeros((train_images.shape[0], NSIS, train_images.shape[2]))\n",
        "        print(train_features.shape)\n",
        "        for segment, image in enumerate(train_images):\n",
        "          for index, val in enumerate(SIS):\n",
        "            train_features[segment][index] = train_images[segment, val-1, :]\n",
        "\n",
        "         # reshape test features to signal image\n",
        "        print(train_features.shape)\n",
        "        test_features = np.zeros((test_images.shape[0], NSIS, test_images.shape[2]))\n",
        "        for segment, image in enumerate(test_images):\n",
        "          for index, val in enumerate(SIS):\n",
        "            test_features[segment][index] = test_images[segment, val-1, :]\n",
        "\n",
        "        # remove last two rows\n",
        "        test_features = test_features[:, :-2, :]\n",
        "        train_features = train_features[:, :-2, :]\n",
        "\n",
        "      else:\n",
        "        train_features = train_images\n",
        "        test_features = test_images\n",
        "\n",
        "      # window labels - only to predict whether gesture or not\n",
        "      test_label_arr = rolling_window(test_labels, M, step, 1)\n",
        "      train_label_arr = rolling_window(train_labels, M, step, 1)\n",
        "\n",
        "      # format labels\n",
        "      train_label_one = [np.max(arr) for arr in train_label_arr]\n",
        "      test_label_one = [np.max(arr) for arr in test_label_arr]\n",
        "\n",
        "      print(np.unique(train_label_one))\n",
        "      print(np.unique(test_label_one))\n",
        "\n",
        "      # save gesture before bringing index back to 1, as it will only be compared - save train / test features to be reloaded for validation\n",
        "      with h5py.File(f'/content/drive/MyDrive/uni/{database}/History_unified/{exercise}_label_history_S{subject}.h5', 'w') as f:\n",
        "        f.create_dataset(f'{exercise}_S{subject}', data=test_label_one, compression='gzip', compression_opts=8)\n",
        "\n",
        "      with h5py.File(f'/content/drive/MyDrive/uni/{database}/History_unified/{exercise}_features_history_S{subject}.h5', 'w') as f:\n",
        "          f.create_dataset(f'{exercise}_S{subject}', data=test_features, compression='gzip', compression_opts=8)\n",
        "\n",
        "      # adjust gestures back to 1-N - it's a list\n",
        "      train_label_ = [x + data_dict[database][f'{exercise}_adjusted'] if x != 0 else 0 for x in train_label_one]\n",
        "      test_label_ = [x + data_dict[database][f'{exercise}_adjusted'] if x != 0 else 0 for x in test_label_one]\n",
        "      print(np.unique(train_label_))\n",
        "      print(np.unique(test_label_))\n",
        "      train_label = one_hot_encoder(train_label_, num_gestures)\n",
        "      test_label = one_hot_encoder(test_label_, num_gestures)\n",
        "\n",
        "      print(\"fitting model now\")\n",
        "\n",
        "      print(train_features.shape, train_label.shape)\n",
        "      print(test_features.shape, test_label.shape)\n",
        "\n",
        "      # Define the model\n",
        "      model = Sequential([\n",
        "\n",
        "              Input(shape=(train_features.shape[1], num_features, 1)),\n",
        "              BatchNormalization(),\n",
        "\n",
        "              Conv2D(64, (3,3), padding='same', strides=(1,1)),\n",
        "              BatchNormalization(),\n",
        "              ReLU(),\n",
        "\n",
        "              Conv2D(64, (3,3), padding='same', strides=(1,1)),\n",
        "              BatchNormalization(),\n",
        "              ReLU(),\n",
        "\n",
        "              LocallyConnectedLayer(64, (1,1)),\n",
        "              BatchNormalization(),\n",
        "              ReLU(),\n",
        "\n",
        "              LocallyConnectedLayer(64, (1,1)),\n",
        "              BatchNormalization(),\n",
        "              ReLU(),\n",
        "\n",
        "              Dropout(0.5),\n",
        "\n",
        "              Flatten(),\n",
        "              Dense(512),\n",
        "              BatchNormalization(),\n",
        "              ReLU(),\n",
        "              Dropout(0.65),\n",
        "              Dense(512),\n",
        "              BatchNormalization(),\n",
        "              ReLU(),\n",
        "              Dropout(0.65),\n",
        "\n",
        "              # predict what gesture out of that exercise - even if subject didn't perform that gesture (for unknown reason) it could be predicted\n",
        "              Dense(num_gestures, activation='softmax'),\n",
        "      ])\n",
        "\n",
        "      model.compile(optimizer=Adam(learning_rate=0.002), loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "      early_stopping = EarlyStopping(monitor='val_accuracy', patience=10)\n",
        "\n",
        "      checkpoint_weights = ModelCheckpoint(f'/content/drive/MyDrive/uni/{database}/Weights_unified/{exercise}_model_weights_S{subject}.weights.h5',\n",
        "                                      save_best_only=True,\n",
        "                                      save_weights_only=True,\n",
        "                                      monitor='val_accuracy',)\n",
        "                                      #verbose=1)\n",
        "\n",
        "      epoch_num = 200\n",
        "      history = model.fit(\n",
        "                train_features,\n",
        "                train_label,\n",
        "                epochs=epoch_num,\n",
        "                batch_size=32,\n",
        "                validation_data=(test_features, test_label),\n",
        "                callbacks=[checkpoint_weights, LearningRateScheduler(lr_schedule), early_stopping],\n",
        "                )\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rllr-WUQPlTO"
      },
      "source": [
        "# Validation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yY-0d1a7y5Tl"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "import seaborn as sns\n",
        "import scipy.sparse as sp\n",
        "\n",
        "tot_num_gestures = data_dict[database]['E1'] + data_dict[database]['E2'] + data_dict[database]['E3'] - 2\n",
        "\n",
        "combined_cm = np.zeros((tot_num_gestures, tot_num_gestures), dtype=int)\n",
        "accuracy_dict = {'E1': [], 'E2': [], 'E3': []}\n",
        "\n",
        "Ns = data_dict[database]['electrodes']\n",
        "SIS, NSIS = generate_signal_permutations(Ns)\n",
        "\n",
        "for exercise in ['E1', 'E2', 'E3']:\n",
        "\n",
        "  num_gestures = data_dict[database][exercise]\n",
        "\n",
        "  # Define the model\n",
        "  model = Sequential([\n",
        "\n",
        "              # CHANGE THIS DEPENDING ON NUMBER OF ROWS IN FEATURES\n",
        "              Input(shape=(NSIS-2, num_features, 1)),\n",
        "              BatchNormalization(),\n",
        "\n",
        "              Conv2D(64, (3,3), padding='same', strides=(1,1)),\n",
        "              BatchNormalization(),\n",
        "              ReLU(),\n",
        "\n",
        "              Conv2D(64, (3,3), padding='same', strides=(1,1)),\n",
        "              BatchNormalization(),\n",
        "              ReLU(),\n",
        "\n",
        "              LocallyConnectedLayer(64, (1,1)),\n",
        "              BatchNormalization(),\n",
        "              ReLU(),\n",
        "\n",
        "              LocallyConnectedLayer(64, (1,1)),\n",
        "              BatchNormalization(),\n",
        "              ReLU(),\n",
        "\n",
        "              Dropout(0.5),\n",
        "\n",
        "              Flatten(),\n",
        "              Dense(512),\n",
        "              BatchNormalization(),\n",
        "              ReLU(),\n",
        "              Dropout(0.65),\n",
        "              Dense(512),\n",
        "              BatchNormalization(),\n",
        "              ReLU(),\n",
        "              Dropout(0.65),\n",
        "\n",
        "              # predict what gesture out of that exercise - even if subject didn't perform that gesture (for unknown reason) it could be predicted\n",
        "              Dense(num_gestures, activation='softmax'),\n",
        "  ])\n",
        "\n",
        "  for subject in range(1, num_subjects+1):\n",
        "\n",
        "      # these subjects in DB3 are not used\n",
        "      if database == 'DB3':\n",
        "        if subject in [6,7]:\n",
        "          continue\n",
        "\n",
        "      # load saved test and train features\n",
        "      with h5py.File(f'/content/drive/MyDrive/uni/{database}/History_unified/{exercise}_features_history_S{subject}.h5', 'r') as f:\n",
        "        test_features = f[f'{exercise}_S{subject}'][:]\n",
        "\n",
        "      with h5py.File(f'/content/drive/MyDrive/uni/{database}/History_unified/{exercise}_label_history_S{subject}.h5', 'r') as f:\n",
        "        test_label = f[f'{exercise}_S{subject}'][:]\n",
        "\n",
        "\n",
        "      # load labels and features for subject\n",
        "      model.load_weights(f'/content/drive/MyDrive/uni/{database}/Weights_unified/{exercise}_model_weights_S{subject}.weights.h5')\n",
        "\n",
        "      print(np.unique(test_label))\n",
        "      # compile the model\n",
        "      predictions = model.predict(test_features)\n",
        "      predicted_classes = np.argmax(predictions, axis=1)\n",
        "\n",
        "      # adjust predicted_classes back to corresponding gesture\n",
        "      #predicted_classes = [(x - data_dict[database][f'{exercise}_adjusted']) for x in predicted_classes]\n",
        "      predicted_classes = [x - data_dict[database][f'{exercise}_adjusted'] if x != 0 else x for x in predicted_classes]\n",
        "      accuracy = accuracy_score(test_label, predicted_classes)\n",
        "      print(f'{exercise} S{subject} Accuracy: {accuracy*100:.2f}%')\n",
        "\n",
        "      # create confusion matrix\n",
        "      for pred, actual in zip(predicted_classes, test_label):\n",
        "        '''\n",
        "        if pred != 0 and actual != 0:\n",
        "          if exercise == 'E2':\n",
        "            pred += 12\n",
        "            actual += 12\n",
        "          elif exercise == 'E3':\n",
        "            pred += 29\n",
        "            actual += 29\n",
        "          '''\n",
        "        combined_cm[int(actual), int(pred)] += 1\n",
        "\n",
        "      accuracy_dict[exercise].append(accuracy*100)\n",
        "\n",
        "for key in accuracy_dict.keys():\n",
        "  print(f'Average accuracy for {key}: {np.mean(accuracy_dict[key])}, and std: {np.std(accuracy_dict[key])}')\n",
        "  print(f'num elements {len(accuracy_dict[key])}')\n",
        "\n",
        "ye = []\n",
        "for val in accuracy_dict.values():\n",
        "  ye.extend(val)\n",
        "print(f'Overall average: {np.mean(ye)} and std: {np.std(ye)}')\n",
        "\n",
        "#cm_path = f'/content/drive/MyDrive/uni/{database}/Confusion_matrices/{database}_confusion_matrix_phin_stft.npz'\n",
        "#sp.save_npz(cm_path, sp.csr_matrix(combined_cm))\n",
        "\n",
        "# adjust labels, because zero is not included and all exercises\n",
        "display_labels = [str(i) for i in range(tot_num_gestures)]\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(combined_cm, annot=False, cmap='inferno', fmt='g', cbar=True, square=True)\n",
        "plt.xticks(ticks=np.arange(0, len(display_labels), 2), labels=[display_labels[i] for i in range(0, len(display_labels), 2)], rotation=45)\n",
        "plt.yticks(ticks=np.arange(0, len(display_labels), 2), labels=[display_labels[i] for i in range(0, len(display_labels), 2)], rotation=45)\n",
        "\n",
        "plt.xlabel('Predicted Labels', fontsize=12)\n",
        "plt.ylabel('True Labels', fontsize=12)\n",
        "plt.title(f\"Classification Accuracy of {database} for {data_dict[database]['subjects']} Subjects\")\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "machine_shape": "hm",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}