{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tnpPytWMhW2g"
      },
      "source": [
        "# Formatting Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "Qs97ICK9M4yq"
      },
      "outputs": [],
      "source": [
        "import scipy.io as sio\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix, ConfusionMatrixDisplay\n",
        "from skimage.feature import hog\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.svm import SVC\n",
        "import os\n",
        "import scipy.sparse as sp\n",
        "import scipy.io as sio\n",
        "import time\n",
        "import cuml\n",
        "from cuml.svm import SVC as cuSVC\n",
        "import cudf\n",
        "from sklearn.decomposition import PCA\n",
        "import scipy.stats\n",
        "import librosa\n",
        "from cuml.neighbors import KNeighborsClassifier as cuKNN\n",
        "import cupy as cp\n",
        "import h5py\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import gc\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Dense, Flatten, BatchNormalization, Dropout, Activation, ReLU, Input, LeakyReLU\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint, LearningRateScheduler\n",
        "from tensorflow.keras import regularizers\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "import tensorflow as tf\n",
        "from sklearn.utils import class_weight\n",
        "from scipy.stats import mode\n",
        "\n",
        "def rolling_window(arr, window_len, step):\n",
        "\n",
        "    num_windows = (len(arr) - window_len) // step + 1\n",
        "    windows = np.zeros((num_windows, window_len), dtype=arr.dtype)\n",
        "\n",
        "    for i in range(num_windows):\n",
        "        start = i * step\n",
        "        end = start + window_len\n",
        "        windows[i] = arr[start:end]\n",
        "\n",
        "    return windows\n",
        "\n",
        "def rolling_window_electrodes(arr, window_len, step):\n",
        "\n",
        "    num_windows = (arr.shape[1] - window_len) // step + 1\n",
        "    windows = np.zeros((arr.shape[0], num_windows, window_len), dtype=arr.dtype)\n",
        "\n",
        "    for i in range(num_windows):\n",
        "      start = i * step\n",
        "      end = start + window_len\n",
        "      windows[:, i] = arr[:, start:end]  # Slice along columns\n",
        "\n",
        "    return windows\n",
        "\n",
        "\n",
        "# turn labels into one hot representation\n",
        "def one_hot_encoder(labels, gestures):\n",
        "\n",
        "        one_hot = np.zeros((len(labels), gestures))\n",
        "\n",
        "        for index, value in enumerate(labels):\n",
        "            label_encode = int(value-1)\n",
        "            one_hot[index][label_encode] = 1\n",
        "\n",
        "        return one_hot\n",
        "\n",
        "class LocallyConnectedLayer(layers.Layer):\n",
        "    def __init__(self, filters, kernel_size, **kwargs):\n",
        "        super(LocallyConnectedLayer, self).__init__(**kwargs)\n",
        "        self.filters = filters\n",
        "        self.kernel_size = kernel_size\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        # Initialize the weights for each spatial location\n",
        "        self.kernel = self.add_weight(\n",
        "            name='kernel',\n",
        "            shape=(self.kernel_size[0], self.kernel_size[1], input_shape[-1], self.filters),\n",
        "            initializer='glorot_uniform',\n",
        "            trainable=True\n",
        "        )\n",
        "        super(LocallyConnectedLayer, self).build(input_shape)\n",
        "\n",
        "    def call(self, inputs):\n",
        "        # Perform the \"local\" convolution manually without weight sharing\n",
        "        return tf.nn.conv2d(inputs, self.kernel, strides=[1, 1, 1, 1], padding='SAME')\n",
        "\n",
        "def lr_schedule(epoch):\n",
        "    initial_lr = 0.002  # Initial learning rate\n",
        "    drop_factor = 0.5  # Factor by which to reduce the learning rate\n",
        "    epochs_drop = 10  # Interval (in epochs) to reduce the learning rate\n",
        "    return initial_lr * (drop_factor ** (epoch // epochs_drop))\n",
        "\n",
        "def generate_signal_permutations(Ns):\n",
        "\n",
        "    SIS = \"\"  # String to store signal indices\n",
        "    NSIS = 1  # Initial length of SIS\n",
        "\n",
        "    # Start with the first signal sequence (hexadecimal '1')\n",
        "    SIS = format(1, 'X')  # SIS starts with '1' in hexadecimal\n",
        "\n",
        "    i = 1\n",
        "    j = i + 1\n",
        "\n",
        "    # Apply the permutation logic as per the original algorithm\n",
        "    while i != j:\n",
        "        if j > Ns:\n",
        "            j = 1  # Wrap around if j exceeds Ns\n",
        "        elif format(i, 'X') + format(j, 'X') not in SIS and format(j, 'X') + format(i, 'X') not in SIS:  # If pair not in SIS\n",
        "            # Append the j-th signal index to SIS (in hexadecimal)\n",
        "            SIS += format(j, 'X')  # Add j to the string in hexadecimal\n",
        "            NSIS += 1  # Increment NSIS\n",
        "            i = j  # Update i to current j\n",
        "            j = i + 1  # Update j to i + 1\n",
        "        else:\n",
        "            j += 1  # Move j to the next signal sequence\n",
        "    # Return the final Signal Index String (SIS) as a list of integers and NSIS\n",
        "    return [int(x, 16) for x in SIS], NSIS\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UJTKipzfPVnO"
      },
      "source": [
        "# TO RUN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2zNT6pi3OnmX",
        "outputId": "79ac0d6f-856b-4b26-dedd-11d7c2753d6b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "from google.colab import drive\n",
        "\n",
        "# ONLY DB1 USES THIS FILE\n",
        "database = 'DB1'\n",
        "\n",
        "# ADJUST ACCORDINLY\n",
        "evaluation = 'HHT'\n",
        "\n",
        "# ADJUST DEPNDING ON WHAT FEATURES\n",
        "num_features = 3\n",
        "\n",
        "# ADJUST DEPNDING ON Signal Image or not\n",
        "signal_image = False\n",
        "\n",
        "# ok, so E2 and E3 adjusted means to adjust them gesture back to 1-N gestures, otherwise it is say 18-40 but depending on what gesture set went first\n",
        "data_dict = {\n",
        "        'DB1': {'E1': 12, 'E2': 17, 'E3': 23, 'E1_adjusted': 0, 'E2_adjusted': 0, 'E3_adjusted': 0, 'fs': 100, 'electrodes': 10, 'subjects': 27, 'train': [1, 3, 4, 6, 7, 8, 9], 'test': [2, 5, 10], 'window length': 20, 'step': 1},\n",
        "        'DB2': {'E1': 18, 'E2': 24, 'E3': 10, 'E1_adjusted': 0, 'E2_adjusted': -17, 'E3_adjusted': -40, 'fs': 100, 'electrodes': 12, 'subjects': 40, 'train': [1, 3, 4, 6], 'test': [2, 5], 'window length': 20, 'step': 1},\n",
        "        'DB3': {'E1': 18, 'E2': 24, 'E3': 10, 'E1_adjusted': 0, 'E2_adjusted': -17, 'E3_adjusted': -40, 'fs': 2000, 'electrodes': 12, 'subjects': 11, 'train': [1, 3, 4, 6], 'test': [2, 5], 'window length': 400, 'step': 20},\n",
        "        'DB5': {'E1': 13, 'E2': 18, 'E3': 24, 'E1_adjusted': 0, 'E2_adjusted': 0, 'E3_adjusted': 0,'fs': 200, 'electrodes': 16, 'subjects': 10, 'train': [1, 3, 4, 6], 'test': [2, 5], 'window length': 40, 'step': 2}\n",
        "        }\n",
        "\n",
        "# use global dictionary\n",
        "num_subjects = data_dict[database]['subjects']\n",
        "fs = data_dict[database]['fs']\n",
        "num_electrodes = data_dict[database]['electrodes']\n",
        "\n",
        "train_trials =  data_dict[database]['train']\n",
        "test_trials = data_dict[database]['test']\n",
        "M, step = data_dict[database]['window length'], data_dict[database]['step']\n",
        "num_freq_bins = int((fs / 2) / (1 / (1/fs * M)))\n",
        "freq_bins = np.linspace(0, fs/2, num_freq_bins)\n",
        "\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "d9EGTBCzB6wG",
        "outputId": "5ad07b4f-bcdc-45c7-94da-8fd32c1db215"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "function ClickConnect(){\n",
              "    console.log(\"Clicked on connect button\");\n",
              "    document.querySelector(\"colab-connect-button\").click()\n",
              "}\n",
              "setInterval(ClickConnect,60000)\n"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "# javascript simulated button click to prevent connection going idle\n",
        "%%javascript\n",
        "function ClickConnect(){\n",
        "    console.log(\"Clicked on connect button\");\n",
        "    document.querySelector(\"colab-connect-button\").click()\n",
        "}\n",
        "setInterval(ClickConnect,60000)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LAdPdauyho1F"
      },
      "source": [
        "# Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 495
        },
        "id": "M-II0F2e2ijE",
        "outputId": "2c406383-3f88-420d-865e-88c4e99434e8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(26052, 10, 3) (11648, 10, 3)\n",
            "[ 1  2  3  4  5  6  7  8  9 10 11 12]\n",
            "[ 1  2  3  4  5  6  7  8  9 10 11 12]\n",
            "fitting model now\n",
            "Epoch 1/200\n",
            "\u001b[1m815/815\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 13ms/step - accuracy: 0.5549 - loss: 1.3902 - val_accuracy: 0.8085 - val_loss: 0.5415 - learning_rate: 0.0020\n",
            "Epoch 2/200\n",
            "\u001b[1m165/815\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.8222 - loss: 0.4989"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-11-e611c44d1148>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    189\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    190\u001b[0m       \u001b[0mepoch_num\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m200\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 191\u001b[0;31m       history = model.fit(\n\u001b[0m\u001b[1;32m    192\u001b[0m                 \u001b[0mtrain_features\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    193\u001b[0m                 \u001b[0mtrain_label\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    115\u001b[0m         \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m             \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/backend/tensorflow/trainer.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq)\u001b[0m\n\u001b[1;32m    370\u001b[0m                     \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    371\u001b[0m                     \u001b[0mlogs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 372\u001b[0;31m                     \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    373\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_training\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    374\u001b[0m                         \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/callbacks/callback_list.py\u001b[0m in \u001b[0;36mon_train_batch_end\u001b[0;34m(self, batch, logs)\u001b[0m\n\u001b[1;32m    170\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_async_dispatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_on_train_batch_end\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 172\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_on_train_batch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    173\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    174\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mon_test_batch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/callbacks/callback_list.py\u001b[0m in \u001b[0;36m_on_train_batch_end\u001b[0;34m(self, batch, logs)\u001b[0m\n\u001b[1;32m    192\u001b[0m         \u001b[0mlogs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpython_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpythonify_logs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    193\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mcallback\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 194\u001b[0;31m             \u001b[0mcallback\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    195\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    196\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_on_test_batch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/callbacks/progbar_logger.py\u001b[0m in \u001b[0;36mon_train_batch_end\u001b[0;34m(self, batch, logs)\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mon_train_batch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_update_progbar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mon_test_batch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/callbacks/progbar_logger.py\u001b[0m in \u001b[0;36m_update_progbar\u001b[0;34m(self, batch, logs)\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mverbose\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 95\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprogbar\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfinalize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     96\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_finalize_progbar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/utils/progbar.py\u001b[0m in \u001b[0;36mupdate\u001b[0;34m(self, current, values, finalize)\u001b[0m\n\u001b[1;32m    161\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_values\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    162\u001b[0m                     avg = backend.convert_to_numpy(\n\u001b[0;32m--> 163\u001b[0;31m                         backend.numpy.mean(\n\u001b[0m\u001b[1;32m    164\u001b[0m                             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_values\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_values\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m                         )\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/backend/tensorflow/numpy.py\u001b[0m in \u001b[0;36mmean\u001b[0;34m(x, axis, keepdims)\u001b[0m\n\u001b[1;32m    586\u001b[0m                 \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgather\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdense_shape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgather_indices\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    587\u001b[0m             )\n\u001b[0;32m--> 588\u001b[0;31m     \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_to_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    589\u001b[0m     \u001b[0mori_dtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstandardize_dtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    590\u001b[0m     \u001b[0mcompute_dtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdtypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"float32\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/backend/tensorflow/core.py\u001b[0m in \u001b[0;36mconvert_to_tensor\u001b[0;34m(x, dtype, sparse)\u001b[0m\n\u001b[1;32m    135\u001b[0m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_to_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 137\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_to_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    138\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSparseTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/util/dispatch.py\u001b[0m in \u001b[0;36mop_dispatch_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1258\u001b[0m       \u001b[0;31m# Fallback dispatch system (dispatch v1):\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1259\u001b[0m       \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1260\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mdispatch_target\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1261\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1262\u001b[0m         \u001b[0;31m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/framework/tensor_conversion.py\u001b[0m in \u001b[0;36mconvert_to_tensor_v2_with_dispatch\u001b[0;34m(value, dtype, dtype_hint, name)\u001b[0m\n\u001b[1;32m    159\u001b[0m     \u001b[0mValueError\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIf\u001b[0m \u001b[0mthe\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0ma\u001b[0m \u001b[0mtensor\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mof\u001b[0m \u001b[0mgiven\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mgraph\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    160\u001b[0m   \"\"\"\n\u001b[0;32m--> 161\u001b[0;31m   return convert_to_tensor_v2(\n\u001b[0m\u001b[1;32m    162\u001b[0m       \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype_hint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype_hint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    163\u001b[0m   )\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/framework/tensor_conversion.py\u001b[0m in \u001b[0;36mconvert_to_tensor_v2\u001b[0;34m(value, dtype, dtype_hint, name)\u001b[0m\n\u001b[1;32m    169\u001b[0m   \u001b[0;34m\"\"\"Converts the given `value` to a `Tensor`.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    170\u001b[0m   \u001b[0;31m# preferred_dtype = preferred_dtype or dtype_hint\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 171\u001b[0;31m   return tensor_conversion_registry.convert(\n\u001b[0m\u001b[1;32m    172\u001b[0m       \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreferred_dtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype_hint\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    173\u001b[0m   )\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/framework/tensor_conversion_registry.py\u001b[0m in \u001b[0;36mconvert\u001b[0;34m(value, dtype, name, as_ref, preferred_dtype, accepted_result_types)\u001b[0m\n\u001b[1;32m    232\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    233\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mret\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 234\u001b[0;31m       \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconversion_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mas_ref\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mas_ref\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    235\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    236\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mret\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNotImplemented\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/framework/constant_tensor_conversion.py\u001b[0m in \u001b[0;36m_constant_tensor_conversion_function\u001b[0;34m(v, dtype, name, as_ref)\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m   \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mas_ref\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mconstant_op\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconstant\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/ops/weak_tensor_ops.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    140\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_auto_dtype_conversion_enabled\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 142\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    143\u001b[0m     \u001b[0mbound_arguments\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msignature\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m     \u001b[0mbound_arguments\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_defaults\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/framework/constant_op.py\u001b[0m in \u001b[0;36mconstant\u001b[0;34m(value, dtype, shape, name)\u001b[0m\n\u001b[1;32m    274\u001b[0m     \u001b[0mValueError\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mcalled\u001b[0m \u001b[0mon\u001b[0m \u001b[0ma\u001b[0m \u001b[0msymbolic\u001b[0m \u001b[0mtensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    275\u001b[0m   \"\"\"\n\u001b[0;32m--> 276\u001b[0;31m   return _constant_impl(value, dtype, shape, name, verify_shape=False,\n\u001b[0m\u001b[1;32m    277\u001b[0m                         allow_broadcast=True)\n\u001b[1;32m    278\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/framework/constant_op.py\u001b[0m in \u001b[0;36m_constant_impl\u001b[0;34m(value, dtype, shape, name, verify_shape, allow_broadcast)\u001b[0m\n\u001b[1;32m    287\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mtrace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"tf.constant\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    288\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_constant_eager_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverify_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 289\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_constant_eager_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverify_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    290\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    291\u001b[0m   const_tensor = ops._create_graph_constant(  # pylint: disable=protected-access\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/framework/constant_op.py\u001b[0m in \u001b[0;36m_constant_eager_impl\u001b[0;34m(ctx, value, dtype, shape, verify_shape)\u001b[0m\n\u001b[1;32m    299\u001b[0m ) -> ops._EagerTensorBase:\n\u001b[1;32m    300\u001b[0m   \u001b[0;34m\"\"\"Creates a constant on the current device.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 301\u001b[0;31m   \u001b[0mt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_to_eager_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mctx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    302\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mshape\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    303\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/framework/constant_op.py\u001b[0m in \u001b[0;36mconvert_to_eager_tensor\u001b[0;34m(value, ctx, dtype)\u001b[0m\n\u001b[1;32m    106\u001b[0m       \u001b[0mdtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdtypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_dtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_datatype_enum\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m   \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 108\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEagerTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    109\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "\n",
        "for exercise in  ['E1', 'E2', 'E3']:\n",
        "\n",
        "  label_dict = {f'S{num}': [] for num in range(1, num_subjects+1)}\n",
        "  features_dict = {f'S{num}': [] for num in range(1, num_subjects+1)}\n",
        "  num_gestures = data_dict[database][exercise]\n",
        "\n",
        "  # for all subjects\n",
        "  for i in range(1,(num_subjects+1)):\n",
        "\n",
        "      # DB1\n",
        "      file = sio.loadmat(f'/content/drive/My Drive/uni/{database}/Electrode Data/S{i}_A1_{exercise}.mat')\n",
        "\n",
        "      label = file['restimulus'].flatten()\n",
        "\n",
        "      # load trial data, so training and testing data can be split according to trials\n",
        "      trials = np.int8(file['rerepetition']).flatten()\n",
        "\n",
        "      emg_data = file['emg'].T\n",
        "\n",
        "      windowed_trials = rolling_window(trials, M, step)\n",
        "      trials_arr = [np.max(arr) for arr in windowed_trials]\n",
        "\n",
        "      train_feature_path = f'/content/drive/My Drive/uni/{database}/Features_down/DATA2_{exercise}_S{i}_features.h5'\n",
        "      with h5py.File(train_feature_path, 'r') as feature_file:\n",
        "\n",
        "          mav = feature_file['MAV'][:]\n",
        "          mavs = feature_file['MAVS'][:]\n",
        "          wap = feature_file['WAP'][:]\n",
        "          zcr = feature_file['ZC'][:]\n",
        "          ar1 = feature_file['ar1'][:]\n",
        "          ar2 = feature_file['ar2'][:]\n",
        "          ar3 = feature_file['ar3'][:]\n",
        "          ar4 = feature_file['ar4'][:]\n",
        "          wl = feature_file['WL'][:]\n",
        "          ssc = feature_file['SSC'][:]\n",
        "          var = feature_file['VAR'][:]\n",
        "          iemg = feature_file['IEMG'][:]\n",
        "          rms = feature_file['RMS'][:]\n",
        "\n",
        "\n",
        "      # load either HHT or STFT data\n",
        "      if evaluation == 'HHT':\n",
        "          hht_path = f'/content/drive/My Drive/uni/{database}/HHT/{exercise}_S{i}_hht.h5'\n",
        "      elif evaluation == 'STFT':\n",
        "          hht_path = f'/content/drive/My Drive/uni/{database}/STFT/{exercise}_S{i}_stft.h5'\n",
        "\n",
        "      with h5py.File(hht_path, 'r') as hht_file:\n",
        "          mean_freq = hht_file['mean freq'][:]\n",
        "          skew_freq = hht_file['skew freq'][:]\n",
        "          psr = hht_file['psr'][:]\n",
        "          #test_imfs = hht_file['num imfs'][:]\n",
        "          peak_freq = hht_file['peak freq'][:]\n",
        "          mean_power = hht_file['mean power'][:]\n",
        "          kurt_freq = hht_file['kurt freq'][:]\n",
        "          var_freq = hht_file['var freq'][:]\n",
        "\n",
        "      if evaluation == 'HHT':\n",
        "          mean_power = np.squeeze(mean_power)\n",
        "          mean_freq = np.squeeze(mean_freq)\n",
        "          psr = np.squeeze(psr)\n",
        "          mean_power = np.squeeze(mean_power)\n",
        "          mean_freq = np.squeeze(mean_freq)\n",
        "          psr = np.squeeze(psr)\n",
        "\n",
        "      # FEATURE SET 1\n",
        "      #feature_matrices= [mav, mavs, wap, wl, ar1, ar2, ar3, ar4, mean_freq, psr]\n",
        "\n",
        "      # FEATURE SET 2\n",
        "      #feature_matrices = [iemg, var, wap, wl, ssc, zcr]\n",
        "\n",
        "      # FEATURE SET 3\n",
        "      feature_matrices = [mean_power, wl, mav]\n",
        "\n",
        "      feature_images = np.stack(feature_matrices, axis=-1).transpose(1, 0, 2)\n",
        "\n",
        "      # if feature image is then create permuation of rows\n",
        "      if signal_image == True:\n",
        "\n",
        "        # generate permuatations using function\n",
        "        Ns = feature_images.shape[1]\n",
        "        SIS, NSIS = generate_signal_permutations(Ns)\n",
        "        print(SIS, NSIS)\n",
        "\n",
        "        # slice features to create Signal Image\n",
        "        features = np.zeros((feature_images.shape[0], NSIS, feature_images.shape[2]))\n",
        "        for segment, image in enumerate(feature_images):\n",
        "          for index, val in enumerate(SIS):\n",
        "              features[segment][index] = feature_images[segment, val-1, :]\n",
        "\n",
        "        # remove last 2 rows (to create even representation)\n",
        "        features = features[:, 1:-1, :]\n",
        "\n",
        "      # if not, just use unchanged array\n",
        "      else:\n",
        "        features = feature_images\n",
        "\n",
        "      # window labels - only to predict whether gesture or not\n",
        "      label_arr = rolling_window(label, M, step)\n",
        "      label_arr = np.array([np.max(arr) for arr in label_arr])\n",
        "\n",
        "      # split train / test data according to trial number\n",
        "      train_index = []\n",
        "      test_index = []\n",
        "      for index, val in enumerate(trials):\n",
        "        if val in train_trials:\n",
        "          train_index.append(index)\n",
        "        elif val in test_trials:\n",
        "          test_index.append(index)\n",
        "\n",
        "      # slice to get training data\n",
        "      train_features = features[train_index, :]\n",
        "      train_label_one = label_arr[train_index]\n",
        "\n",
        "      # slice to get test data\n",
        "      test_features = features[test_index, :]\n",
        "      test_label_one = label_arr[test_index]\n",
        "\n",
        "      print(train_features.shape, test_features.shape)\n",
        "\n",
        "      # save gesture before bringing index back to 1, as it will only be compared - save these for reloading for validation\n",
        "      # this is used, since Colab kept on crashing\n",
        "      with h5py.File(f'/content/drive/MyDrive/uni/{database}/History_unified/{exercise}_label_history_S{i}.h5', 'w') as f:\n",
        "        f.create_dataset(f'{exercise}_S{i}', data=test_label_one, compression='gzip', compression_opts=8)\n",
        "\n",
        "      with h5py.File(f'/content/drive/MyDrive/uni/{database}/History_unified/{exercise}_features_history_S{i}.h5', 'w') as f:\n",
        "          f.create_dataset(f'{exercise}_S{i}', data=test_features, compression='gzip', compression_opts=8)\n",
        "\n",
        "      # adjust gestures back to 1-N - it's a list, since some datasets require adjusting\n",
        "      train_label_ = [x + data_dict[database][f'{exercise}_adjusted'] for x in train_label_one]\n",
        "      test_label_ = [x + data_dict[database][f'{exercise}_adjusted'] for x in test_label_one]\n",
        "\n",
        "      print(np.unique(train_label_))\n",
        "      print(np.unique(test_label_))\n",
        "\n",
        "      # one encode labels\n",
        "      train_label = one_hot_encoder(train_label_, num_gestures)\n",
        "      test_label = one_hot_encoder(test_label_, num_gestures)\n",
        "\n",
        "      print(\"fitting model now\")\n",
        "\n",
        "      # Define the model\n",
        "      model = Sequential([\n",
        "\n",
        "              Input(shape=(train_features.shape[1], num_features, 1)),\n",
        "              BatchNormalization(),\n",
        "\n",
        "              Conv2D(64, (3,3), padding='same', strides=(1,1)),\n",
        "              BatchNormalization(),\n",
        "              ReLU(),\n",
        "\n",
        "              Conv2D(64, (3,3), padding='same', strides=(1,1)),\n",
        "              BatchNormalization(),\n",
        "              ReLU(),\n",
        "\n",
        "              LocallyConnectedLayer(64, (1,1)),\n",
        "              BatchNormalization(),\n",
        "              ReLU(),\n",
        "\n",
        "              LocallyConnectedLayer(64, (1,1)),\n",
        "              BatchNormalization(),\n",
        "              ReLU(),\n",
        "\n",
        "              Dropout(0.5),\n",
        "\n",
        "              Flatten(),\n",
        "              Dense(512),\n",
        "              BatchNormalization(),\n",
        "              ReLU(),\n",
        "              Dropout(0.65),\n",
        "              Dense(512),\n",
        "              BatchNormalization(),\n",
        "              ReLU(),\n",
        "              Dropout(0.65),\n",
        "\n",
        "              # predict what gesture out of that exercise - even if subject didn't perform that gesture (for unknown reason) it could be predicted\n",
        "              Dense(num_gestures, activation='softmax'),\n",
        "\n",
        "\n",
        "      ])\n",
        "\n",
        "      model.compile(optimizer=Adam(learning_rate=0.002), loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "      early_stopping = EarlyStopping(monitor='val_accuracy', patience=15)\n",
        "\n",
        "      checkpoint_weights = ModelCheckpoint(f'/content/drive/MyDrive/uni/{database}/Weights_unified/{exercise}_model_weights_S{i}.weights.h5',\n",
        "                                      save_best_only=True,\n",
        "                                      save_weights_only=True,\n",
        "                                      monitor='val_accuracy',)\n",
        "                                      #verbose=1)\n",
        "\n",
        "      epoch_num = 200\n",
        "      history = model.fit(\n",
        "                train_features,\n",
        "                train_label,\n",
        "                epochs=epoch_num,\n",
        "                batch_size=32,\n",
        "                validation_data=(test_features, test_label),\n",
        "                callbacks=[checkpoint_weights, LearningRateScheduler(lr_schedule), early_stopping],\n",
        "                )\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rllr-WUQPlTO"
      },
      "source": [
        "# Validation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yY-0d1a7y5Tl"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "import seaborn as sns\n",
        "import scipy.sparse as sp\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "tot_num_gestures = data_dict[database]['E1'] + data_dict[database]['E2'] + data_dict[database]['E3']\n",
        "\n",
        "combined_cm = np.zeros((tot_num_gestures, tot_num_gestures), dtype=int)\n",
        "accuracy_dict = {'E1': [], 'E2': [], 'E3': []}\n",
        "\n",
        "# redo just in case it timed out\n",
        "Ns = data_dict[database]['electrodes']\n",
        "SIS, NSIS = generate_signal_permutations(Ns)\n",
        "\n",
        "for exercise in ['E1', 'E2', 'E3']:\n",
        "\n",
        "  num_gestures = data_dict[database][exercise]\n",
        "\n",
        "  # Define the model\n",
        "  model = Sequential([\n",
        "\n",
        "              # CHANGE INPUT SHAPE AS APPROPRIATE\n",
        "              Input(shape=(NSIS-2, num_features, 1)),\n",
        "              BatchNormalization(),\n",
        "\n",
        "              Conv2D(64, (3,3), padding='same', strides=(1,1)),\n",
        "              BatchNormalization(),\n",
        "              ReLU(),\n",
        "\n",
        "              Conv2D(64, (3,3), padding='same', strides=(1,1)),\n",
        "              BatchNormalization(),\n",
        "              ReLU(),\n",
        "\n",
        "              LocallyConnectedLayer(64, (1,1)),\n",
        "              BatchNormalization(),\n",
        "              ReLU(),\n",
        "\n",
        "              LocallyConnectedLayer(64, (1,1)),\n",
        "              BatchNormalization(),\n",
        "              ReLU(),\n",
        "\n",
        "              Dropout(0.5),\n",
        "\n",
        "              Flatten(),\n",
        "              Dense(512),\n",
        "              BatchNormalization(),\n",
        "              ReLU(),\n",
        "              Dropout(0.65),\n",
        "              Dense(512),\n",
        "              BatchNormalization(),\n",
        "              ReLU(),\n",
        "              Dropout(0.65),\n",
        "\n",
        "              # predict what gesture out of that exercise - even if subject didn't perform that gesture (for unknown reason) it could be predicted\n",
        "              Dense(num_gestures, activation='softmax'),\n",
        "  ])\n",
        "\n",
        "  for i in range(1, num_subjects+1):\n",
        "\n",
        "      # reload train / test features\n",
        "      with h5py.File(f'/content/drive/MyDrive/uni/{database}/History_unified/{exercise}_features_history_S{i}.h5', 'r') as f:\n",
        "        test_features = f[f'{exercise}_S{i}'][:]\n",
        "\n",
        "      with h5py.File(f'/content/drive/MyDrive/uni/{database}/History_unified/{exercise}_label_history_S{i}.h5', 'r') as f:\n",
        "        test_label = f[f'{exercise}_S{i}'][:]\n",
        "\n",
        "\n",
        "      # load labels and features for subject\n",
        "      model.load_weights(f'/content/drive/MyDrive/uni/{database}/Weights_unified/{exercise}_model_weights_S{i}.weights.h5')\n",
        "\n",
        "      print(np.unique(test_label))\n",
        "      # compile the model\n",
        "      predictions = model.predict(test_features)\n",
        "      predicted_classes = np.argmax(predictions, axis=1)\n",
        "\n",
        "      accuracy = accuracy_score(test_label-1, predicted_classes)\n",
        "      print(f'{exercise} S{i} Accuracy: {accuracy*100:.2f}%')\n",
        "\n",
        "      # create confusion matrix\n",
        "      for pred, actual in zip(predicted_classes, test_label):\n",
        "        #adjust as it is zero indexed and there''s no at-rest gesture\n",
        "        if exercise == 'E2':\n",
        "          pred += 11\n",
        "          actual += 11\n",
        "        elif exercise == 'E3':\n",
        "          pred += 28\n",
        "          actual += 28\n",
        "        combined_cm[int(actual), int(pred)] += 1\n",
        "\n",
        "      accuracy_dict[exercise].append(accuracy*100)\n",
        "\n",
        "for key in accuracy_dict.keys():\n",
        "  print(f'Average accuracy for {key}: {np.mean(accuracy_dict[key])}, and std: {np.std(accuracy_dict[key], ddof=1)}')\n",
        "  print(f'num elements {len(accuracy_dict[key])}')\n",
        "\n",
        "ye = []\n",
        "for val in accuracy_dict.values():\n",
        "  ye.extend(val)\n",
        "print(f'Overall average: {np.mean(ye)} and std: {np.std(ye, ddof=1)}')\n",
        "\n",
        "cm_path = f'/content/drive/MyDrive/uni/{database}/Confusion_matrices/{database}_confusion_matrix_stft.npz'\n",
        "sp.save_npz(cm_path, sp.csr_matrix(combined_cm))\n",
        "\n",
        "# adjust labels, because zero is not included and all exercises - gesture 1 indexed from 0 as there is not at-rest gesture!!\n",
        "display_labels = [str(i) for i in range(1, tot_num_gestures+1)]\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(combined_cm, annot=False, cmap='inferno', fmt='g', cbar=True, square=True)\n",
        "plt.xticks(ticks=np.arange(0, len(display_labels), 2), labels=[display_labels[i] for i in range(0, len(display_labels), 2)], rotation=45)\n",
        "plt.yticks(ticks=np.arange(0, len(display_labels), 2), labels=[display_labels[i] for i in range(0, len(display_labels), 2)], rotation=45)\n",
        "\n",
        "plt.xlabel('Predicted Labels', fontsize=12)\n",
        "plt.ylabel('True Labels', fontsize=12)\n",
        "plt.title(f\"Classification Accuracy of {database} for {data_dict[database]['subjects']} Subjects\")\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}